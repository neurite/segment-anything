{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15832489-be4d-4b28-9ab4-87d31274004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pillow_avif\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import (\n",
    "    normalize,\n",
    "    pad,\n",
    "    resize,\n",
    "    to_pil_image,\n",
    ")\n",
    "\n",
    "from segment_anything import sam_model_registry, SamPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bfe1d-50ce-4ec6-81e7-60ea1824cf32",
   "metadata": {},
   "source": [
    "### The image encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be35aabc-2cbe-4bca-86b2-26851082c4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['default', 'vit_h', 'vit_l', 'vit_b'])\n"
     ]
    }
   ],
   "source": [
    "print(sam_model_registry.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9afb113-087f-4ab2-947e-f608beae3f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sam(\n",
      "  (image_encoder): ImageEncoderViT(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-31): 32 x Block(\n",
      "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (neck): Sequential(\n",
      "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): LayerNorm2d()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (3): LayerNorm2d()\n",
      "    )\n",
      "  )\n",
      "  (prompt_encoder): PromptEncoder(\n",
      "    (pe_layer): PositionEmbeddingRandom()\n",
      "    (point_embeddings): ModuleList(\n",
      "      (0-3): 4 x Embedding(1, 256)\n",
      "    )\n",
      "    (not_a_point_embed): Embedding(1, 256)\n",
      "    (mask_downscaling): Sequential(\n",
      "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): LayerNorm2d()\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (no_mask_embed): Embedding(1, 256)\n",
      "  )\n",
      "  (mask_decoder): MaskDecoder(\n",
      "    (transformer): TwoWayTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TwoWayAttentionBlock(\n",
      "          (self_attn): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_token_to_image): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLPBlock(\n",
      "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_image_to_token): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_attn_token_to_image): Attention(\n",
      "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (iou_token): Embedding(1, 256)\n",
      "    (mask_tokens): Embedding(4, 256)\n",
      "    (output_upscaling): Sequential(\n",
      "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): GELU(approximate='none')\n",
      "    )\n",
      "    (output_hypernetworks_mlps): ModuleList(\n",
      "      (0-3): 4 x MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (iou_prediction_head): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Default is vit_h\n",
    "sam = sam_model_registry['default'](checkpoint=\"../models/sam_vit_h_4b8939.pth\")\n",
    "print(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "672eefbc-dabc-40c8-83c3-53950d8debc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageEncoderViT(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x Block(\n",
      "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (neck): Sequential(\n",
      "    (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): LayerNorm2d()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (3): LayerNorm2d()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "image_encoder = sam.image_encoder\n",
    "print(image_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b56bd-c3c0-4921-ab35-750dcf249b88",
   "metadata": {},
   "source": [
    "### Transform images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dfbbe69-0ba2-4ac2-b564-d77ae27ca0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_mode': 'RGB', 'image_size': 1024, 'pixel_mean': tensor([[[123.6750]],\n",
      "\n",
      "        [[116.2800]],\n",
      "\n",
      "        [[103.5300]]]), 'pixel_std': tensor([[[58.3950]],\n",
      "\n",
      "        [[57.1200]],\n",
      "\n",
      "        [[57.3750]]]), 'device': device(type='cpu')}\n"
     ]
    }
   ],
   "source": [
    "model_image_params = {\n",
    "    'image_mode': sam.image_format,\n",
    "    'image_size': image_encoder.img_size,\n",
    "    'pixel_mean': sam.pixel_mean,\n",
    "    'pixel_std': sam.pixel_std,\n",
    "    'device': sam.device,\n",
    "}\n",
    "print(model_image_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613f9bf5-6860-48c1-9b9b-0cf749c4ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def resize_longest_side(\n",
    "    image: np.ndarray,\n",
    "    target_length: int,\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    Resizes images so that the longest side is resized to the target length.\n",
    "    Arguments:\n",
    "      image: of shape HWC\n",
    "      target_length:\n",
    "      image_format:\n",
    "    \"\"\"\n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "    longest_side = max(h, w)\n",
    "    scale = target_length * 1.0 / longest_side\n",
    "    # Adjust +0.5 to compensate int() flooring effect\n",
    "    newh = int(h * scale + 0.5)\n",
    "    neww = int(w * scale + 0.5)\n",
    "    # Convert\n",
    "    #   tensor of shape CHW\n",
    "    #   a numpy ndarray of shape HWC\n",
    "    # to\n",
    "    #   a PIL Image\n",
    "    pil_image = to_pil_image(image)\n",
    "    resized_image = np.array(resize(pil_image, [newh, neww]))\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16a081ff-37af-4ee2-a1d0-4995f0823479",
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def to_image_tensor(\n",
    "    image: np.ndarray,\n",
    "    device: str,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      image (np.ndarray): The image for calculating masks. Expects an\n",
    "        image in HWC uint8 format, with pixel values in [0, 255].\n",
    "    \"\"\"\n",
    "    image_torch = torch.as_tensor(image, device=device).float()\n",
    "    # permute(2, 0, 1): HWC -> CHW\n",
    "    # coniguous() rearranges elements linearly in memory; it is usually necessary after the permute() call\n",
    "    # [None, :, :, :] adds a dimension for batch\n",
    "    # TODO: einops + continguous\n",
    "    image_torch = image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "    return image_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb7f7ec-3bd8-41df-82de-f8ee35bfb655",
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "@torch.no_grad()\n",
    "def compute_embeddings(\n",
    "    image: np.ndarray,\n",
    "    image_mode: str = 'RGB',\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the image embeddings for the provided image.\n",
    "    Arguments:\n",
    "      image: np.ndarray\n",
    "        Expects an image in HWC uint8 format with pixel values in [0, 255].\n",
    "    \"\"\"\n",
    "    assert image_mode in [\n",
    "        'RGB',\n",
    "        'BGR',\n",
    "    ]\n",
    "    if image_mode != model_image_params['image_mode']:\n",
    "        # Reverse on the last dimension which is the color channel\n",
    "        # In NumPy, Ellipsis (...) is a shorthand for selecting all preceding dimensions\n",
    "        # This is equivalent to image[:, :, ::-1]\n",
    "        # ::-1 is indexing start:stop:step where step -1 reverses the array\n",
    "        image = image[..., ::-1]\n",
    "\n",
    "    image = resize_longest_side(image, model_image_params['image_size'])\n",
    "    image_tensor = to_image_tensor(image, device=model_image_params['device'])\n",
    "\n",
    "    # normalize() expects image tensor CHW or BCHW\n",
    "    image_tensor = normalize(\n",
    "        image_tensor,\n",
    "        model_image_params['pixel_mean'].squeeze(),\n",
    "        model_image_params['pixel_std'].squeeze(),\n",
    "    )\n",
    "\n",
    "    # Skip BC to read HW\n",
    "    h, w = image_tensor.shape[-2:]\n",
    "    padh = model_image_params['image_size'] - h\n",
    "    padw = model_image_params['image_size'] - w\n",
    "    # Pad right and bottom\n",
    "    image_tensor = pad(image_tensor, (0, 0, padw, padh))\n",
    "\n",
    "    image_embeddings = image_encoder(image_tensor)\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab98767-6575-47de-8959-0e60f9cad570",
   "metadata": {},
   "source": [
    "### Verify embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3307f2e2-7aa3-4c7b-ab5a-4c378084f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('../../../fashion/images/brands/erdem-01.webp')\n",
    "image = np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac26816-f9b7-4c15-b61f-7d7165cef58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.5 s, sys: 11.5 s, total: 1min 10s\n",
      "Wall time: 7.16 s\n"
     ]
    }
   ],
   "source": [
    "%time embeddings = compute_embeddings(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "732fd824-121b-4155-8ea1-ccfe8cc89488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "487a76f2-9d86-4829-9fb9-f968795c38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eab9c8f4-348b-4ccb-80bc-5b01e1563b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.6 s, sys: 8.73 s, total: 1min 5s\n",
      "Wall time: 6.59 s\n"
     ]
    }
   ],
   "source": [
    "%time predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bdb3163-070d-4373-9485-690fa27334f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.is_image_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6b148df-dd85-411f-8953-e7dfe5f08c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(embeddings, predictor.features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "sam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
